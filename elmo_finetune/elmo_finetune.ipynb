{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import preprocessor as p\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "\n",
    "p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.NUMBER, p.OPT.RESERVED, p.OPT.SMILEY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/nlp-getting-started/train.csv')\n",
    "test = pd.read_csv('../data/nlp-getting-started/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cleaning 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(tweet):\n",
    "    tweet = p.clean(tweet)\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    \n",
    "    tweet = tweet.replace('...', ' ... ')\n",
    "    if '...' not in tweet:\n",
    "        tweet = tweet.replace('..', ' ... ')\n",
    "\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt, Komoran, Mecab, Hannanum, Kkma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(tokenizer_name):\n",
    "    if tokenizer_name == \"komoran\":\n",
    "        tokenizer = Komoran()\n",
    "    elif tokenizer_name == \"okt\":\n",
    "        tokenizer = Okt()\n",
    "    elif tokenizer_name == \"mecab\":\n",
    "        tokenizer = Mecab()\n",
    "    elif tokenizer_name == \"hannanum\":\n",
    "        tokenizer = Hannanum()\n",
    "    elif tokenizer_name == \"kkma\":\n",
    "        tokenizer = Kkma()\n",
    "    elif tokenizer_name == \"khaiii\":\n",
    "        tokenizer = KhaiiiApi()\n",
    "    else:\n",
    "        tokenizer = Mecab()\n",
    "    return tokenizer\n",
    "\n",
    "def post_processing(tokens):\n",
    "    results = []\n",
    "    for token in tokens:\n",
    "        # 숫자에 공백을 주어서 띄우기\n",
    "        processed_token = [el for el in re.sub(r\"(\\d)\", r\" \\1 \", token).split(\" \") if len(el) > 0]\n",
    "        results.extend(processed_token)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "수정 필요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullTokenizer(object):\n",
    "    def __init__(self, vocab_file, do_lower_case=True):\n",
    "        self.vocab = load_vocab(vocab_file)\n",
    "        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n",
    "        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        split_tokens = []\n",
    "        for token in self.basic_tokenizer.tokenize(text):\n",
    "            for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
    "                split_tokens.append(sub_token)\n",
    "\n",
    "        return split_tokens\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return convert_by_vocab(self.vocab, tokens)\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        return convert_by_vocab(self.inv_vocab, ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이프라인 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tweet_classifier_model(object):\n",
    "    def __init__(self,\n",
    "                train_corpus_fname=None,  # 훈련 말뭉치\n",
    "                tokenized_train_corpus_fname=None, # 토크나이징 된 훈련 말뭉치\n",
    "                test_corpus_fname=None,  # 테스트 말뭉치\n",
    "                tokenized_test_corpus_fname=None,  # 토크나이징 된 테스트 말뭉치\n",
    "                model_name='bert',\n",
    "                model_save_path=None,\n",
    "                vocab_fname=None,\n",
    "                eval_every=1000,\n",
    "                batch_size=32, num_epoch=10, dropout_keep_prob_rate=0.9,\n",
    "                model_ckpt_path=None):\n",
    "    \n",
    "        # 변수 구성\n",
    "        self.model_name = model_name\n",
    "        self.eval_every = eval_every\n",
    "        self.model_ckpt_path = model_ckpt_path\n",
    "        self.model_save_path = model_save_path\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epoch = num_epoch\n",
    "        self.dropout_keep_prob_rate = dropout_keep_prob_rate\n",
    "        self.best_valid_score = 0.0\n",
    "\n",
    "        # tokenizer 정의\n",
    "        if self.model_name == 'bert':\n",
    "            self.tokenizer = FullTokenizer(vocab_file=vocab_fname, do_lower_case=False)\n",
    "        else:\n",
    "            self.tokenizer = get_tokenizer('mecab')\n",
    "\n",
    "        # 말뭉치 불러오기 & 토크나이징\n",
    "        self.train_data, self.train_data_size = self.load_or_tokenize_corpus(train_corpus_fname, tokenized_train_corpus_fname)\n",
    "        self.test_data, self.test_data_size = self.load_or_tokenize_corpus(test_corpus_fname, tokenized_test_corpus_fname)\n",
    "    \n",
    "    # 말뭉치 불러오기 & 토크나이징\n",
    "    def load_or_tokenize_corpus(self, corpus_fname, tokenized_corqus_fname):\n",
    "        \"\"\"\n",
    "        말뭉치를 불러와 형태소 분석(토크나이징)하는 함수\n",
    "        클래스가 선언됨과 동시에 호출됨.\n",
    "        tokenized_corpus_fname 경로에 데이터가 존재하면 해당 경로의 데이터를 읽어들이고, 그렇지 않으면 corus_fname 경로의 데이터를 읽어서 형태소 분석을 실시\n",
    "        \"\"\"\n",
    "    \n",
    "        data_set = []\n",
    "        if os.path.exists(tokenized_corqus_fname):\n",
    "            tf.compat.v1.logging.info('토크나이징 말뭉치 : ' + tokenized_corqus_fnamen)\n",
    "            with open(tokenized_corqus_fname, 'r') as file1:\n",
    "                for line in file1:\n",
    "                    # \\u241E : Symbol for Record Seperator\n",
    "                    tokens, label = line.strip().split('\\u241E')\n",
    "                    if len(tokens) > 0:\n",
    "                        data_set.append([tokens.split(\" \"), int(label)])\n",
    "        else:\n",
    "            with open(corpus_fname, 'r') as file2:\n",
    "                next(file2)  # skip head line\n",
    "                for line in file2:\n",
    "                    sentence, label = line.strip().splitit('\\u241E')\n",
    "                    if self.model_name == 'bert':\n",
    "                        tokens = self.tokenizer.tokenize(sentence)\n",
    "                    else:\n",
    "                        tokens = self.tokenizer.morphs(sentence)\n",
    "                        tokens = post_processing(tokens)\n",
    "                    if int(label) >= 1:\n",
    "                        int_label = 1\n",
    "                    else:\n",
    "                        int_label = 0\n",
    "                    data_set.append([tokens, int_label])\n",
    "            with open(tokenized_corqus_fname, 'w') as file3:\n",
    "                for token, label in data_set:\n",
    "                    file3.writelinest(' '.join(tokens) + '\\u241E' + str(label) + '\\n')\n",
    "        \n",
    "        return data_set, len(data_set)\n",
    "    \n",
    "    def get_batch(self, data, num_epoch, is_training=True):\n",
    "        if is_training:\n",
    "            data_size = self.train_data_size\n",
    "        else:\n",
    "            data_size = self.test_data_size\n",
    "            \n",
    "        num_batches_per_epoch = int((data_size - 1) / self.batch_size) + 1\n",
    "        for epoch in range(num_epoch):\n",
    "            idx = random.sample(range(data_size), data_size)\n",
    "            data = np.array(data)[idx]\n",
    "            for batch_num in range(num_batches_per_epoch):\n",
    "                batch_sentences = []\n",
    "                batch_labels = []\n",
    "                start_index = batch_num * self.batch_size\n",
    "                end_index = min((batch_num + 1) * self.batch_size, data_size)\n",
    "                features = data[start_index:end_index]\n",
    "                \n",
    "                for feature in features:\n",
    "                    sentence, label = feature\n",
    "                    batch_sentences.append(sentence)\n",
    "                    batch_labels.append(int(label))\n",
    "                yield self.make_input(batch_sentences, batch_labels, is_training)\n",
    "    \n",
    "    def train(self, sess, saver, global_step, output_feed):\n",
    "        train_batches = self.get_batch(self.train_data, self.num_epoch, is_training=True)\n",
    "        \n",
    "        checkpoint_loss = 0.0\n",
    "        for current_intput_feed in train_batches:\n",
    "            _, _, _, current_loss = sess.run(output_feed, current_intput_feed)\n",
    "            checkpoint_loss += current_loss\n",
    "            if global_step.eval(sess) % self.eval_every == 0:\n",
    "                tf.compat.v1.logging.info('global step %d train loss %.4f' %(global_step.eval(sess), checkpoint_loss / self.eval_every))\n",
    "                checkpoint_loss = 0.0\n",
    "                self.validation(sess, saver, global_step)\n",
    "    \n",
    "    def validation(self, sess, saver, global_step):\n",
    "        valid_loss, valid_pred, valid_num_data = 0, 0, 0\n",
    "        output_feed = [self.logits, self.loss]\n",
    "        test_batches = self.get_batch(self.test_data,\n",
    "                                     num_epoch=1,\n",
    "                                     is_training=False)\n",
    "        for current_intput_feed, current_labels in test_batches:\n",
    "            current_logits, current_loss = sess.run(output_feed, current_intput_feed)\n",
    "            current_preds = np.argmax(current_logitst, axis=-1)\n",
    "            valid_loss += current_loss\n",
    "            valid_num_data += len(current_labels)\n",
    "            for pred, label in zip(current_predst, current_labels):\n",
    "                if pred == label:\n",
    "                    valid_pred += 1\n",
    "        \n",
    "        valid_score = valid_pred / valid_num_datad\n",
    "        \n",
    "        tf.compat.v1.logging.info('valid loss %.4f valid score %.4f' % (valid_loss, valid_score))\n",
    "        \n",
    "        if valid_score > self.best_valid_score:\n",
    "            selff.best_valid_score = valid_score\n",
    "            path = self.model_save_path + '/' + str(valid_score)\n",
    "            saver.save(sess, path, global_step=global_step)\n",
    "    \n",
    "    def make_input(self, sentence, labels, is_training):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def tune(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ELMo 파인 튜닝 네트워크 그래프\n",
    "https://github.com/allenai/bilm-tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bilm.model import BidirectionalLanguageModel\n",
    "from bilm.data import Batcher\n",
    "from bilm.elmo import weight_layers\n",
    "\n",
    "def make_elmo_graph(options_fname,\n",
    "                    pretrain_model_fname,\n",
    "                    max_characters_per_token,\n",
    "                    num_labels,\n",
    "                    tune=False):\n",
    "    \"\"\"\n",
    "    ids_placeholder : ELMo 네트워크의 입력값 (ids)\n",
    "        - shape : [batch_size, unroll_steps, max_character_byte_length]\n",
    "    elmo_embeddings : fine tuning 네트워크의 입력값 (ELMo 네트워크의 출력값)\n",
    "        - shape : [batch_size, unroll_steps, dimension]\n",
    "    labels_placeholder : fine tuning 네트워크의 출력값 (예 : 긍정=1/부정=0)\n",
    "        - shape : [batch_size]\n",
    "    loss : fine tuning 네트워크의 loss\n",
    "    \"\"\"\n",
    "        # biLM graph 구축\n",
    "    # pretrained ELMo model 불러오기\n",
    "    bilm = BidirectionalLanguageModel(options_fname, pretrain_model_fname)\n",
    "    # bilm 에 placeholder 입력\n",
    "    ids_placeholder = tf.placeholder(tf.int32,\n",
    "                                     shape=(None, None, max_characters_per_token),\n",
    "                                     name = 'input')\n",
    "    \n",
    "    if tune:\n",
    "        labels_placeholder = tf.placeholder(tf.int32,\n",
    "                                           shape=(None))\n",
    "    else:\n",
    "        labels_placeholder = None\n",
    "    \n",
    "    embeddings_op = bilm(ids_placeholder)\n",
    "    input_lengths = embeddings_op['lengths']\n",
    "    \n",
    "    if tune:\n",
    "        dropout_keep_prob = tf.placeholder(tf.float32, name='dropout_keep_prob')\n",
    "    else:\n",
    "        dropout_keep_prob = tf.constant(1.0, dtype=tf.float32)\n",
    "        \n",
    "    # ELMo layer\n",
    "    # shape = [batch_size, unroll_steps, dimension]\n",
    "    elmo_embeddings = weight_layers('elmo_embeddings',\n",
    "                                    embeddings_op,\n",
    "                                    l2_coef=0.0,\n",
    "                                    use_top_only=False,\n",
    "                                    do_layer_norm=True)\n",
    "    \n",
    "    # fine_tuning network의 입력\n",
    "    features = tf.nn.dropout(elmo_embeddings['weighted_op'], dropout_keep_prob)\n",
    "    # Bidirectional LSTM\n",
    "    lstm_cell_fw = tf.compat.v1.nn.rnn_cell.LSTMCell(num_units=512,\n",
    "                                                     cell_clip=5,\n",
    "                                                     proj_clip=5)\n",
    "    lstm_cell_bw = tf.compat.v1.nn.rnn_cell.LSTMCell(num_units=512,\n",
    "                                                     cell_clip=5,\n",
    "                                                     proj_clip=5)\n",
    "    lstm_output, _ = tf.compat.v1.nn.bidirectional_dynamic_rnn(cell_fw=lstm_cell_fw,\n",
    "                                                               cell_bw=lstm_cell_bw,\n",
    "                                                               inputs=features,\n",
    "                                                               sequence_length=input_lengths,\n",
    "                                                               dtype=float32)\n",
    "    \n",
    "    # Attention Layer\n",
    "    output_fw, output_bw = lstm_output\n",
    "    H = tf.contrib.layers.fully_connected(inputs = output_fw + output_bw,\n",
    "                                          num_outputs = 256,\n",
    "                                          activation_fn = tf.nn.tanh)\n",
    "    attention_score = tf.nn.softmax(tf.contrib.layers.fully_connected(inputs=H, num_outputs=1, activation_fn=None), axis=1)\n",
    "    attention_output = tf.squeeze(tf.matmul(tf.transpose(H, perm=[0, 2, 1]), attention_score), axis=-1)\n",
    "    layer_output = tf.nn.dropout(attention_output, dropout_keep_prob)\n",
    "    \n",
    "    # Feed-Forward Layer\n",
    "    fc = tf.contrib.layers.fully_connected(inputs=layer_output,\n",
    "                                           num_outputs=512,\n",
    "                                           activation_fn=tf.nn.relu,\n",
    "                                           weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           biases_initializer=tf.zeros_initializer())\n",
    "    features_drop = tf.nn.dropout(fc, dropout_keep_prob)\n",
    "    logits = tf.contrib.layers.fully_connected(inputs=features_drop,\n",
    "                                               num_outputs=num_labels,\n",
    "                                               activation_fn=None,\n",
    "                                               weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                               biases_initializer=tf.zeros_initializer())\n",
    "    if tune:\n",
    "        # Loss Layer\n",
    "        CE = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels_placeholder, logits=logits)\n",
    "        loss = tf.reduce_mean(CE)\n",
    "        return ids_placeholder, labels_placeholder, dropout_keep_prob, logits, loss\n",
    "    else:\n",
    "        # prob Layer\n",
    "        probs = tf.nn.softmax(logits, axis=-1, name='probs')\n",
    "        return ids_placeholder, elmo_embeddings, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pretrained - elmo 불러오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo = hub.load(\"https://tfhub.dev/google/elmo/3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* elmo\n",
    "* 훈련 txt\n",
    "* 테스트 txt\n",
    "* elmo-vocab.txt\n",
    "* elmo.model\n",
    "* options.json\n",
    "\n",
    "model_save_path \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
